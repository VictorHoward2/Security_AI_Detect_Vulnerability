{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31194, 3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_full = pd.read_csv('./CVEFixes-kaggle.csv')\n",
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c = df_full[df_full['language'] == 'c']\n",
    "df_c.shape\n",
    "# lấy ngẫu nhiên 800 trường hợp trong dataset\n",
    "data = df_c.sample(n=800)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "# Tải tokenizer và model codeBERT\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CodeVulnerabilityClassifier(nn.Module):\n",
    "    def __init__(self, codebert_model):\n",
    "        super(CodeVulnerabilityClassifier, self).__init__()\n",
    "        self.codebert = codebert_model\n",
    "        for param in self.codebert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.fc1 = nn.Linear(768, 256)  # 768 is the hidden state size of codeBERT\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.codebert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]  # Get the hidden state of the [CLS] token\n",
    "        out1 = torch.relu(self.fc1(pooled_output))  # Apply ReLU activation to the first linear layer output\n",
    "        logits = self.fc2(out1)  # Pass the output through the second linear layer\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chia tập dữ liệu thành tập train và tập test (giả sử data là dataframe chứa 'code' và 'safety' fields)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tiền xử lý dữ liệu và chuyển đổi thành dạng tensor sử dụng tokenizer\n",
    "train_texts = train_data['code'].tolist()\n",
    "train_labels = [0 if label == 'safe' else 1 for label in train_data['safety']]\n",
    "\n",
    "test_texts = test_data['code'].tolist()\n",
    "test_labels = [0 if label == 'safe' else 1 for label in test_data['safety']]\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(train_labels))\n",
    "test_dataset = torch.utils.data.TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], torch.tensor(test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd2545c48a74a2c8a9d8db8805e33ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7119, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.8167, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7215, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6934, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7132, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6878, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7470, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6746, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6735, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7115, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6877, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7010, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6945, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6927, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7064, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6947, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6788, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6696, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7276, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7088, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6975, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6814, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6985, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6923, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6903, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6936, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6946, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6966, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6937, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6908, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7011, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6942, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6895, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6864, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6692, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6619, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6957, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6737, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6744, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7256, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7090, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6299, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7695, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7336, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6886, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6617, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7065, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7034, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7369, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6861, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6929, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6964, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6899, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6926, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Huấn luyện mô hình\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "classifier = CodeVulnerabilityClassifier(model)\n",
    "classifier.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(classifier.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "for epoch in tqdm(range(3), total=3):\n",
    "    for input_ids, attention_mask, labels in train_loader:\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = classifier(input_ids, attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.53125\n"
     ]
    }
   ],
   "source": [
    "# Đánh giá mô hình trên tập test\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for input_ids, attention_mask, labels in test_loader:\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        logits = classifier(input_ids, attention_mask)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy on test set: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
